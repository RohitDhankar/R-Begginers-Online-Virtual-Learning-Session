---
title: "R for Beginners - R for Finance Code File-1"
output:
  pdf_document: default
  html_notebook: default
  html_document: default
urlcolor: blue
---


This R code book written by [Rohit Dhankar](https://github.com/RohitDhankar) . GitHub - https://github.com/RohitDhankar 

Code and Data > https://github.com/RohitDhankar/R-Beginners-Online-Virtual-Learning-Session    

Good practice to keep track of current Working Directory , list all Objects in R ENVIRONMENT - specially so when committing changes to Git or any other version control Remote directory. 

# R for Finance

A univariate data set having ONE Variable that varies as time passes is a TIME SERIES.  
Most data sets used for Machine Learning or Advanced Statistics have Multivariate Data.  

TIME SERIES data is used as inferential and predictive data for various Business  
functions. Forecasting the Weather phenomenon for Agri based business.  
Forecasting usage of Pesticides , onset of Rains etc etc .    

Within FINANCE usage is for Predicting STOCK ASSET VALUEs and also NAV's   
of Mutual Funds.  



```{r}

library("forecast");
library("ggplot2");
library("ggfortify") 
library("tseries")


```

```{r}
# Set Seed -- ensure reproducible results 

set.seed(123)

infy_df <- read.csv("~/Desktop/R_Own/R_Finance/DATA_Files/INFY.csv")
str(infy_df)
summary(infy_df)
start_date <- infy_df$Date[1] ## [1] 19-Aug-2015
len_df<-length(infy_df$Date)
end_date <-infy_df$Date[len_df] ## [1] 17-Aug-2017

# Convert DF to TS 

ts_infy <-ts(infy_df, start = c(2015,8), end =  c(2017,8),  frequency = 12) #
plot.ts(ts_infy,main="NSE-INFY-STOCK_PRICE_DAILY_CLOSING",col="green")

#
#


```

```{r}
#?ts() ## Uncomment to seek help 
# frequency	---
# the number of observations per unit of time.
# deltat	---
# the fraction of the sampling period between successive 
# observations; e.g., 1/12 for monthly data. Only one of frequency 
# or deltat should be provided.
#
ts_infy1 <-ts(infy_df, start = c(2015,8), end =  c(2017,8), frequency = 24,names = "NSE-INFY-STOCK_PRICE_DAILY_CLOSING") #
#
str(ts_infy1);summary(ts_infy1)
#
typeof(ts_infy1) ## [1] "double"
#
typeof(ts_infy) ## [1] "double"
#
class(ts_infy)
#
head(ts_infy1);head(time(ts_infy1));tail(time(ts_infy1)) 
# Head of TS # view Head - sampled times # view Tail of sampled times
dim(ts_infy1);
#
tsdisplay(ts_infy1)
#
plot.ts(ts_infy1,main="NSE-INFY-STOCK_PRICE_DAILY_CLOSING",col="blue",xlab="YEAR + MONTH",ylab="INFY- DAILY_CLOSING")


```

# STATIONARY TIME SERIES :- 

We need to make the TIME SERIES Stationary if it is Not already so.  
TBD ---- WIP --- ARMA and ARIMA models   


# ARMA(p,q)

TBD -------



# Auto Regressive Integrated Moving Averages (ARIMA)

TBD -------




# Decomposition of TIME SERIES :- 

" to construct, from an observed time series, a number of   
component series (that could be used to reconstruct the original by additions   
or multiplications) where each of these has a certain characteristic or type of behaviour.  

SOURCE -- WIKI -- https://en.wikipedia.org/wiki/Decomposition_of_time_series

 There are many lively discussions with regards which functions from the STATS  
 package to be used for decomposition i am refering a few below here - i shall  
 experiment with a couple of methods and see what works best for us.   
 

```{r}
#



```
Various Sources ---  

STACK_EXCHANGE --  https://stats.stackexchange.com/questions/9506/stl-trend-of-time-series-using-r?rq=1  

STACK_EXCHANGE --  https://stats.stackexchange.com/questions/85987/which-is-better-stl-or-decompose  




# ESTIMATING and ELIMINATING THE TREND 

We need to identify , then estimate and eliminate the trend component if present  
in the Time Series Model .We also endevaour to eliminate the other TWO   
Deterministic Features of the TS MODEL   

We are dealing with DISCRETE data of Stock Price Closing Values as   
taken from the NSE on 18 AUG 17   

SOURCE --- http://userwww.sfsu.edu/efc/classes/biol710/timeseries/TimeSeriesAnalysis.html

Learning pointers to be highlighted from the above cited source - 

STATIONARY TIME SERIES DATA - 

Definitions -- TBD 

NON STATIONARY TIME SERIES DATA - 

Quoting verbatim from the source mentioned above -- 
"Box and Jenkins developed the AutoRegressive Integrative Moving Average (ARIMA) model  
which combined the AutoRegresive (AR) and Moving Average (MA) models developed earlier  
with a differencing factor that removes in trend in the data.
"

Further Reading -- Multiple Sources :- 
WIKI --- 


```{r}
#?stl() # stl {stats}
# Decompose a time series into seasonal, trend and irregular 
# components using loess, acronym STL.

require(graphics)

plot(stl(ts_infy1, "per"))
plot(stl(ts_infy1, s.window = 7, t.window = 50, t.jump = 1))

#plot(stllc <- stl(log(co2), s.window = 21))
#summary(stllc)


```

In the plot above --Vertical Grey Bars - seen on right of each plot.  
These signify relative importance Component.  
Longest Vertical bar is of - TREND  
In general terms - if the GREY BAR is Larger   
The impact of the Component in this case TREND is LEAST   
###Source  -- https://stats.stackexchange.com/questions/7876/interpreting-range-bars-in-rs-plot-stl


```{r}
dcomp_infy_1 <-stl(ts_infy1,s.window = 12,t.window = 10) ;head(dcomp_infy_1$time.series) ;str(dcomp_infy_1) 

```



```{r}

typeof(dcomp_infy_1) # LIST 
summary(dcomp_infy_1)

# We focus on the IQR Values --- 

# IQR:
#      STL.seasonal STL.trend STL.remainder data 
#      23.25        37.02     10.43         45.65
#    %  50.9         81.1      22.9         100.0



```



```{r}

plot(dcomp_infy_1,main="Additive Decomposition -> Data[Yt],Seasonal[St],Trend[Mt] and Remainder[Et]",col="blue") 

```



```{r}

acf(dcomp_infy_1$time.series[,3],main="Auto Correlation Function - ACF - Residuals")



```



```{r}

Resid_decomp<-dcomp_infy_1$time.series[,3] # Time Series of Residuals 
acf(Resid_decomp,main="Auto Correlation Function - ACF - Residuals [Resid_decomp]")


```



```{r}

Season_decomp<-dcomp_infy_1$time.series[,2] # Time Series of Seasonal Components 
plot(Season_decomp,col="red", cex = 5,main="NSE-INFY_Seasonal Decomposition- 2015 to 2017")
lines(stats::lowess(Season_decomp),col="blue")
#lines(stats::lowess(Season_decomp),col="blue")


```



```{r}

# The stl() function presumes an ADDITIVE Decomposition 
# We now take a Natural Log [ Log-transform] the response to give a MULTIPLICATIVE Decomposition

require(ggplot2)
require(ggfortify)
autoplot(stl(ts_infy1, s.window = 12, t.window = 10), ts.colour = 'blue',main="NSE-INFY-2015 to 2017 - Additive Decomposition ")


```

# ETS - Exponential Smoothing Methods 

We highly recommend further reads for the ETS   
There are some refrences at the end of this text.  

## Source -- https://www.otexts.org/fpp/7/6


```{r}

#lag.plot(ts_infy1,lags = 1,main="NSE-INFY-2015 to AUG 2017 ") ;gglagplot(ts_infy1,lags = 1) # Lag plots INFY - LAG ==1

```



```{r}

# TBD --- 

ggfreqplot(ts_infy1,main="NSE-INFY--Month Wise ___Overall Data"); ## Not intelligent for this data set 
ggfreqplot(ts_infy1,freq=4,main="NSE-INFY--Quarterly___Overall Data")


```



```{r}

# 



 
```






```{r}




```



```{r}

# Start the clock!
# ptm <- proc.time()
# 
# vec_gross_sale <- p_sale_count_rnd*p_sale_cost_rnd
# 
# summary(vec_gross_sale)
# 
# proc.time() - ptm 
# 
#
# As seen below in our case 
# ELAPSED time - 1st 0.011 , 2nd - 0.012
# Thus the WALL CLOCK or REAL / ELAPSED 
# timings are almost same . 
#
# The USER TIME and SYSTEM TIME's in our case 
# add upto - 
# 1st - 0.008
# 2nd - 0.012

# Thus it would seem we are better off 
# with Vector Multiplication

# But we also need to consider 
# once we have the "vec_gross_sale" 
# we will need to add it to out "mdf"

# Kindly also note the Timings will
# differ for each system - also for each run 
# of the chunk of code on same sys

# Definition of user Time --- The ‘user time’ is the CPU time 
# charged for execution of user instructions of the calling process.
#
# REFER- https://stat.ethz.ch/R-manual/R-devel/library/base/html/proc.time.html



```


```{r}

# Now to multiply TWO Columns of the DF 
# Also called COLUMNAR VECTORS 

# Again start the clock!
# ptm <- proc.time()
# 
# mdf$gross_sale<- mdf$p_sale_count_rnd*mdf$p_sale_cost_rnd
# 
# proc.time() - ptm 
# #
# str(mdf)
# #
# summary(mdf)
# #
# write.csv(mdf,file="Mkt_DATA_Files/mdf.csv") 
# ## Writes to Sub Directory - DATA_Files
# #


```


```{r}



```




```{r}


```



```{r}


```



```{r}


```




```{r}


```




```{r}


```




```{r}


```




```{r}


```




```{r}


```




```{r}


```




```{r}


```




```{r}


```
